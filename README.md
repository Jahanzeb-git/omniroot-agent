# ğŸ¤– Omniroot Agent

[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://hub.docker.com/r/jahanzeb833/omniroot-agent)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-121212?style=for-the-badge&logo=chainlink&logoColor=white)](https://langchain.com/)
[![React](https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=for-the-badge&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Version](https://img.shields.io/badge/version-v1.3.0-blue?style=for-the-badge)](https://github.com/Jahanzeb-git/omniroot-agent)

> **ğŸ›¡ï¸ SECURE CONTAINERIZED ENVIRONMENT**: This agent operates within a secure Docker container with isolated access to its own environment. Safe for development and testing without compromising host system security.

## ğŸ¯ Overview

**Omniroot Agent** is a sophisticated, containerized Agentic AI application built with LangChain and LiteLLM that provides autonomous task execution with multi-LLM support. Designed for secure development environments, this agent combines the power of multiple AI models with containerized access to solve complex coding tasks, development workflows, and automation challenges.

## ğŸ–¼ï¸ Application Preview

[![Screenshot-from-2025-06-06-17-41-21.png](https://i.postimg.cc/fWPzRCKD/Screenshot-from-2025-06-06-17-41-21.png)](https://postimg.cc/SjWFZW3v)

*Omniroot Agent's integrated development environment featuring VS Code Web IDE, terminal interface, multi-LLM chat, and real-time system monitoring capabilities.*

## âœ¨ Key Features

### ğŸ§  **Multi-LLM Intelligence Hub**
- **OpenAI Models**: GPT-4, GPT-o1, GPT-o1-mini, GPT-o3
- **Anthropic Claude**: Claude 3.5 Sonnet, Claude 3.7 Sonnet, Claude 4 Opus, Claude 4 Sonnet
- **Google Gemini**: Gemini 2.5 Pro, Gemini 1.5 Pro
- **Open Source**: Together AI's Qwen 2.5 Coder Instruct 32B, Qwen 3, Deepseek V3
- **Extensible**: Easy integration of additional models via LiteLLM

### ğŸ” **Containerized Development Environment**
- Secure isolated container environment
- Full filesystem operations within container scope
- Shell command execution with container-level access
- Project creation and development server management
- File editing and code manipulation capabilities
- No host system access - maintains security boundaries

### ğŸ’¾ **Advanced Memory Management**
- **Contextual Memory Condensation**: Custom algorithm using LangChain's ConversationSummaryMemory
- **Dynamic Context Optimization**: Maximizes context retention while minimizing token usage
- **Session Persistence**: Resume conversations and workflows across container restarts
- **Workflow State Management**: Maintains task context and execution state

### ğŸ–¥ï¸ **Integrated Development Environment**
- **VS Code Web Interface**: Full-featured code editor accessible via browser (Port 8080)
- **Live Terminal Access**: Real-time command execution with streaming output
- **Development Servers**: Instant preview and testing of web applications
- **Frontend Preview**: Live preview tabs for development projects

### ğŸ”„ **Real-Time Communication**
- **Server-Sent Events (SSE)**: Streaming responses and real-time updates
- **WebSocket Integration**: Bidirectional communication for terminal and file operations
- **REST API**: Comprehensive HTTP endpoints for all agent functionalities
- **Event-Driven Architecture**: Asynchronous task execution and status updates

### ğŸ› ï¸ **Custom Tool Ecosystem**
- **Shell Tool**: Container-level command execution with safety validations
- **File Tools**: Advanced read/write operations within container environment
- **Email Tool**: SMTP integration for notifications and reporting
- **Development Tools**: Project scaffolding, server management, and code analysis
- **Extensible Framework**: Easy integration of custom tools and capabilities

### ğŸš¨ **Intelligent Safety Systems**
- **Dynamic Prompting**: Adaptive prompt engineering based on execution context
- **Command Validation**: Pre-execution safety checks and confirmations
- **Container Isolation**: Secure boundaries preventing host system access
- **Risk Assessment**: Automated evaluation of command impact within container scope

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Omniroot Agent Container (v1.3.0)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Frontend Layer   â”‚                 Backend Layer                         â”‚
â”‚   (React/TypeScript)â”‚               (Python/FastAPI)                        â”‚
â”‚     Port: 5173      â”‚                Port: 5001                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ VS Code Web IDE   â”‚ â€¢ LangChain Agent Engine                              â”‚
â”‚   (Port: 8080)      â”‚ â€¢ Multi-LLM Router (LiteLLM)                          â”‚
â”‚ â€¢ Terminal Interfaceâ”‚ â€¢ Memory Condensation Layer                           â”‚
â”‚ â€¢ Real-time Chat UI â”‚ â€¢ SSE Streaming Server                                â”‚
â”‚ â€¢ Session Managementâ”‚ â€¢ WebSocket Event Handler                             â”‚
â”‚ â€¢ Settings Panel    â”‚ â€¢ REST API Endpoints                                  â”‚
â”‚ â€¢ Preview Tabs      â”‚ â€¢ Development Server Manager                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚                                   â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
   â”‚   LLM   â”‚                    â”‚  Tools  â”‚                        â”‚  Memory   â”‚
   â”‚ Router  â”‚                    â”‚ Engine  â”‚                        â”‚ Manager   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                              â”‚                                   â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”
 â”‚      â”‚      â”‚            â”‚         â”‚         â”‚                    â”‚     â”‚     â”‚
â–¼      â–¼      â–¼            â–¼         â–¼         â–¼                    â–¼     â–¼     â–¼
OpenAI  Claude  Gemini    Shell    File     Email              Context Summary Session
GPT-4   Models  Pro      Tool     Tools    Tool               Memory  Engine  Store
Together.AI     â”‚         â”‚        â”‚        â”‚                   â”‚       â”‚       â”‚
Deepseek V3     â”‚         â”‚        â”‚        â”‚                   â”‚       â”‚       â”‚
                â”‚         â”‚        â”‚        â”‚                   â”‚       â”‚       â”‚
                â–¼         â–¼        â–¼        â–¼                   â–¼       â–¼       â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                Container Environment Access                         â”‚
           â”‚  â€¢ Container FS: /app/workspace  â€¢ File Operations                  â”‚
           â”‚  â€¢ Shell Commands              â€¢ Development Servers               â”‚
           â”‚  â€¢ Process Control             â€¢ Project Management                â”‚
           â”‚  â€¢ Secure Boundaries           â€¢ Code Execution                    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Communication Flow
1. **Frontend** sends requests via REST API or WebSocket
2. **Backend** processes through LangChain agent with selected LLM
3. **Tools Engine** executes operations within container environment
4. **Memory Manager** condenses and stores conversation context
5. **SSE Stream** delivers real-time updates to frontend
6. **Container Environment** provides isolated access to development resources

## ğŸ“ Project Structure

```
Omniroot-agent/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ agent.py                 # Core LangChain agent logic
â”‚   â”œâ”€â”€ app.py                   # FastAPI application & SSE endpoints
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ agent_prompt.py      # Dynamic agent system prompts
â”‚   â”‚   â””â”€â”€ summary_prompt.py    # Memory condensation prompts
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ shell_tool.py        # Container command execution
â”‚   â”‚   â”œâ”€â”€ file_tools.py        # File system read/write operations
â”‚   â”‚   â”œâ”€â”€ email_tool.py        # SMTP email integration
â”‚   â”‚   â”œâ”€â”€ terminal_events.py   # Terminal WebSocket handling
â”‚   â”‚   â””â”€â”€ user_shell.py        # Interactive shell interface
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ llm_utils.py         # LLM provider management
â”‚   â”‚   â”œâ”€â”€ memory_utils.py      # Context optimization algorithms
â”‚   â”‚   â”œâ”€â”€ db_utils.py          # Session and data persistence
â”‚   â”‚   â”œâ”€â”€ system_utils.py      # Container integration utilities
â”‚   â”‚   â”œâ”€â”€ service_utils.py     # Development service management
â”‚   â”‚   â”œâ”€â”€ extract_utils.py     # Data extraction and parsing
â”‚   â”‚   â””â”€â”€ codeserver.py        # VS Code server integration
â”‚   â”œâ”€â”€ watchers/
â”‚   â”‚   â””â”€â”€ file_watcher.py      # Real-time file system monitoring
â”‚   â””â”€â”€ thread_storage.py        # Thread-safe storage management
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/          # React UI components
â”‚   â”‚   â”œâ”€â”€ contexts/            # React context providers
â”‚   â”‚   â”œâ”€â”€ hooks/               # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ services/            # API communication services
â”‚   â”‚   â”œâ”€â”€ store/               # State management
â”‚   â”‚   â”œâ”€â”€ types/               # TypeScript type definitions
â”‚   â”‚   â””â”€â”€ utils/               # Frontend utility functions
â”‚   â””â”€â”€ [build configuration]
â”œâ”€â”€ Dockerfile                   # Multi-stage container build
â”œâ”€â”€ docker-compose.yml          # Complete service orchestration
â””â”€â”€ entrypoint.sh               # Container initialization script
```

## ğŸš€ Quick Start

### Prerequisites

- **Docker**: Version 20.0+ with BuildKit support
- **Available Ports**: 5001 (Backend API), 5173 (Frontend UI), 8080 (VS Code)
- **API Keys**: For your chosen LLM providers
- **System Requirements**: Minimum 4GB RAM, 2GB available disk space

### One-Command Installation

```bash
# Pull and run the latest version
docker run -d --name omniroot-agent -p 5173:5173 -p 5001:5001 -p 8080:8080 jahanzeb833/omniroot-agent:latest
# Stop the container
docker stop omniroot-agent
# Start the container again
docker start omniroot-agent
```

### Alternative: Docker Compose (Recommended)

```bash
# Create project directory
mkdir omniroot-agent && cd omniroot-agent

# Download docker-compose.yml
curl -O https://raw.githubusercontent.com/Jahanzeb-git/omniroot-agent/main/docker-compose.yml

# Start the application
docker-compose up -d
```

### Development Setup

```bash
# Clone the repository
git clone git@github.com:Jahanzeb-git/omniroot-agent.git
cd omniroot-agent

# Build and run locally
docker-compose up --build
```

### Access the Application

```
ğŸŒ Frontend Interface: http://localhost:5173
ğŸ”— Backend API: http://localhost:5001
ğŸ’» VS Code IDE: http://localhost:8080
```

## âš™ï¸ Configuration

### Initial Setup

1. **Open the Application** at `http://localhost:5173`

2. **Navigate to Settings** (âš™ï¸ icon in the sidebar)

3. **Configure LLM Provider**:
   ```
   Model Selection: Choose from dropdown (GPT-4, Claude, Gemini, etc.)
   API Key: Enter your provider's API key
   Temperature: 0.1 (recommended for consistent agent behavior)
   Max Tokens: 4000 (adjust based on your needs)
   Max Iterations: 10 (prevents infinite loops)
   ```

4. **Memory Management**:
   ```
   Summarization Percentage: 70% (balance context vs. cost)
   Max Context Length: 16000 tokens
   Session Persistence: Enabled
   ```

5. **Email Integration** (Optional):
   ```
   SMTP Server: smtp.gmail.com
   Port: 587
   Email: your-email@gmail.com
   App Password: your-app-specific-password
   Display Name: Omniroot Agent
   ```

6. **Save Configuration** and refresh the page

## ğŸ’¡ Usage Examples

### Development Workflow
```
User: "Create a new React TypeScript project with Tailwind CSS and start development server"

Agent: I'll set up a complete React TypeScript project with Tailwind CSS.

ğŸ“ Creating project structure in /app/workspace...
ğŸ“¦ Installing dependencies...
ğŸ¨ Configuring Tailwind CSS...
ğŸš€ Starting development server on port 3000
âœ… Project ready! Access it via the Preview tab or VS Code IDE
```

### File Management & Coding
```
User: "Create a Python FastAPI application with user authentication"

Agent: I'll create a FastAPI application with authentication features.

ğŸ“ Creating project structure...
ğŸ” Setting up JWT authentication...
ğŸ“Š Adding user management endpoints...
ğŸ§ª Creating test files...
âš™ï¸ Configuring environment variables...
âœ… FastAPI application ready! Run with: uvicorn main:app --reload
```

### Multi-Step Automation
```
User: "Analyze this CSV file and create a dashboard with charts"

Agent: I'll analyze your CSV data and create an interactive dashboard.

ğŸ“Š Reading and analyzing CSV data...
ğŸ“ˆ Generating statistical insights...
ğŸ¨ Creating React dashboard with charts...
ğŸ“± Setting up responsive design...
ğŸš€ Starting dashboard server...
âœ… Dashboard ready at http://localhost:3001
```

## ğŸ“Š Performance Metrics

| Metric | Value | Notes |
|--------|-------|-------|
| **Container Startup Time** | 15-30 seconds | Initial container initialization |
| **LLM Response Time** | 1-10 seconds | Varies by model complexity and API latency |
| **Memory Usage** | 1-4 GB | Depends on active projects and context size |
| **Container Size** | ~2 GB | Includes all dependencies and tools |
| **Concurrent Operations** | 5-10 | Per container instance |
| **Context Retention** | 85-95% | With memory condensation enabled |
| **File Operations** | <500ms | For standard read/write operations |
| **Development Server Start** | 2-10 seconds | Varies by project type and size |

### Performance Notes
- **Response times vary significantly based on chosen LLM model**
- **GPT-4 and Claude models typically provide faster responses than GPT-o1 series**
- **Open-source models via Together AI may have higher latency**
- **Memory condensation helps maintain performance in long conversations**

## ğŸ›¡ï¸ Security & Safety

### Container Security Model
- **Isolated Environment**: All operations occur within the secure Docker container
- **No Host Access**: Container cannot access or modify host system files
- **Network Isolation**: Limited network access based on container configuration
- **Process Isolation**: Container processes are isolated from host processes
- **Resource Limits**: Configurable CPU and memory limits prevent resource exhaustion

### Safe for Development
- **File Operations**: Limited to container workspace directory
- **Command Execution**: Restricted to container environment
- **Network Operations**: Outbound API calls only (for LLM providers)
- **No Privilege Escalation**: Standard user permissions within container

## ğŸ”§ Advanced Usage

### Custom Tool Development

```python
# Example: Custom tool for database operations
from langchain.tools import BaseTool
from typing import Optional

class CustomDatabaseTool(BaseTool):
    name = "database_query"
    description = "Execute database queries within container"
    
    def _run(self, query: str) -> str:
        # Your custom tool implementation
        return result
```

### Memory Configuration

```python
# Adjust memory condensation behavior
MEMORY_CONFIG = {
    "summarization_percentage": 0.7,  # Keep 70% of context
    "max_context_length": 16000,      # Maximum tokens
    "condensation_trigger": 0.8,      # Trigger at 80% capacity
    "preserve_recent_messages": 5     # Always keep last 5 messages
}
```

## ğŸ” Troubleshooting

### Common Issues

#### Container Fails to Start
```bash
# Check if ports are available
netstat -tulpn | grep -E ':(5173|5001|8080)'

# Check container logs
docker logs omniroot-agent

# Ensure sufficient resources
docker system df
```

#### LLM Provider Issues
- **Verify API key format and validity**
- **Check rate limits and billing status**
- **Ensure model availability in your region**
- **Test with different models if one fails**

#### Performance Issues
```bash
# Monitor container resources
docker stats omniroot-agent

# Restart container if needed
docker restart omniroot-agent
```

### Debug Mode

Enable detailed logging:

```bash
docker run -e LOG_LEVEL=DEBUG jahanzeb833/omniroot-agent:latest
```

## ğŸ› ï¸ Development & Contribution

### Local Development Setup

1. **Clone Repository**
   ```bash
   git clone git@github.com:Jahanzeb-git/omniroot-agent.git
   cd omniroot-agent
   ```

2. **Backend Development**
   ```bash
   cd Backend
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   python app.py
   ```

3. **Frontend Development**
   ```bash
   cd Frontend
   npm install
   npm run dev
   ```

4. **Build and Test**
   ```bash
   docker build -t omniroot-agent:dev .
   docker run -d -p 5173:5173 -p 5001:5001 -p 8080:8080 omniroot-agent:dev
   ```

### Contributing Guidelines

We welcome contributions! Priority areas include:

- **Enhanced LLM integrations and optimizations**
- **Additional development tools and frameworks**
- **Performance improvements and caching**
- **UI/UX enhancements**
- **Documentation and tutorials**
- **Bug fixes and stability improvements**

### Development Workflow

1. **Fork** the repository on GitHub
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Develop** your changes with tests
4. **Test** thoroughly in container environment
5. **Document** your changes
6. **Commit** with descriptive messages
7. **Push** to your branch
8. **Create** a Pull Request

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **LangChain** for the powerful agent framework
- **LiteLLM** for seamless multi-provider LLM integration
- **FastAPI** for high-performance API development
- **React & TypeScript** for modern frontend development
- **Docker** for containerization and deployment
- **VS Code** for the integrated development environment
- **Open Source Community** for continuous inspiration and contributions

## ğŸ“ Support & Community

- **ğŸ› Bug Reports**: [GitHub Issues](https://github.com/Jahanzeb-git/omniroot-agent/issues)
- **ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/Jahanzeb-git/omniroot-agent/discussions)
- **ğŸ“– Documentation**: [Wiki](https://github.com/Jahanzeb-git/omniroot-agent/wiki)

### Development Status

**Current Version**: v1.3.0 (Beta)

This is a beta release suitable for development and testing. While stable for most use cases, some bugs may exist. Performance depends on chosen LLM model and API response times.

---

<div align="center">

**â­ Star this repository if you find it useful!**

**ğŸ¤ Contributions are welcome and appreciated!**

**ğŸ” Secure containerized environment for safe development!**

[![GitHub stars](https://img.shields.io/github/stars/Jahanzeb-git/omniroot-agent.svg?style=social&label=Star)](https://github.com/Jahanzeb-git/omniroot-agent)
[![GitHub forks](https://img.shields.io/github/forks/Jahanzeb-git/omniroot-agent.svg?style=social&label=Fork)](https://github.com/Jahanzeb-git/omniroot-agent/fork)
[![Docker Pulls](https://img.shields.io/docker/pulls/jahanzeb833/omniroot-agent.svg)](https://hub.docker.com/r/jahanzeb833/omniroot-agent)

</div>

---

> **ğŸ’¡ Note**: Omniroot Agent v1.3.0 operates within a secure containerized environment, making it safe for development and testing without compromising host system security. This beta release is suitable for coding projects, automation tasks, and AI-assisted development workflows.
